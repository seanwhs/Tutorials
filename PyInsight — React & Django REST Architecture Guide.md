# ğŸ“˜ PyInsight â€” Architecture, Async Flow & Deployment

---

## 1ï¸âƒ£ High-Level Architecture Overview

PyInsight is built on a **modern asynchronous stack**, designed to handle multi-gigabyte CSV files without blocking the UI. By decoupling data ingestion from analytics, users get **real-time partial results** while heavy computations execute asynchronously.

```
ğŸŸ¦ USER
â””â”€ Upload CSV & Select Columns
      â”‚
      â–¼
ğŸŸ© FRONTEND (React/TypeScript)
â”œâ”€ Upload CSV file
â”œâ”€ Select columns, filters, and options
â”œâ”€ POST /api/analyze() â†’ submit analysis request
â”œâ”€ Poll API or subscribe to WebSocket events
â””â”€ Render JSON, tables, charts, and visualizations
      â”‚
      â–¼
ğŸŸ¨ BACKEND API (Django REST)
â”œâ”€ Validate CSV structure, format, and required columns
â”œâ”€ Return async task_id to frontend
â””â”€ Schedule Celery task for asynchronous processing
      â”‚
      â–¼
ğŸŸª CELERY WORKER
â”œâ”€ Compute CSV summaries: stats, aggregations, correlations
â”œâ”€ Evaluate configurable business rules (rules.yaml)
â”œâ”€ Execute plugins or ML pipelines for advanced analytics
â””â”€ Save incremental progress (Redis) and final results (MySQL/PostgreSQL)
      â”‚
      â–¼
ğŸŸ« STORAGE
â”œâ”€ Redis (ephemeral progress tracking & pub/sub)
â””â”€ MySQL/PostgreSQL (persistent storage, audits, historical analytics)
      â”‚
      â–¼
ğŸŸ§ WEBSOCKET / CHANNELS
â”œâ”€ Push incremental updates to subscribed clients
â””â”€ Notify dashboard in real-time
      â”‚
      â–¼
ğŸŸ© DASHBOARD UI
â”œâ”€ Render live summaries and interactive tables
â”œâ”€ Highlight rule violations & alerts
â”œâ”€ Show plugin outputs, charts, and heatmaps
â””â”€ Provide dynamic filtering, sorting, and column selection
```

**Key Highlights:**

* Each component is **decoupled**, enabling maintainability and scalability.
* **Parallel plugin/ML execution** allows modular extensions.
* **Incremental updates** via WebSocket + Redis provide a reactive UI.
* Supports **interactive visualizations**, letting users explore partial results immediately.

---

## 2ï¸âƒ£ Async Timeline & Component Flow
The following sequence highlights the non-blocking nature of the system. The user never waits for the "Processing" spinner to finish before seeing the first data points.
```
Time â†’
ğŸŸ¦ USER       ğŸŸ© FRONTEND        ğŸŸ¨ BACKEND API      ğŸŸª CELERY WORKER      ğŸŸ« STORAGE       ğŸŸ§ WEBSOCKET      ğŸŸ© DASHBOARD
----        --------        -----------      -------------     ---------       ---------        ---------
  |               |                |                  |                 |                |               |
  | Upload CSV    |                |                  |                 |                |               |
  |-------------->|                |                  |                 |                |               |
  |               | POST /api/analyze()           |                  |                |               |
  |               |------------------------------->| Validate CSV & columns|              |               |
  |               |                                |----------------->| Schedule Celery task|             |
  |               | Receive task_id                |                  | Store initial state |             |
  |               |<-------------------------------|                  |               |               |
  | Poll / WS     | GET /api/status / WS event     |                  |               |               |
  |-------------->|------------------------>|                        |                 |               |
  | Render partial|                                |                  | Push incremental updates ----->| Render partial results|
  â–¼               â–¼                                â–¼                  â–¼                 â–¼               â–¼               â–¼
Final Analytics  Live JSON / Table Updates   Async task execution    CSV summarization &   Redis = ephemeral  WS pushes partial   Dashboard renders
& Rule Violations updates on Dashboard       (task_id returned)     rules evaluation      MySQL = persistent + final results      analytics & plugin outputs
```

**Notes:**

* Frontend immediately receives `task_id` â†’ **non-blocking UX**
* Heavy tasks run asynchronously in Celery â†’ **async offloading**
* Redis + WebSocket â†’ **incremental updates**
* MySQL/PostgreSQL â†’ **persistent final results & audit**

---

## 3ï¸âƒ£ Backend Component Sequence
The backend follows a strict pipeline to ensure data integrity before any computation begins.
```
CSV Upload
  â”‚
  â–¼
API View (AnalyzeCSV)
  â”œâ”€ validators.py â†’ Checks CSV structure, required headers, data types
  â”œâ”€ serializers.py â†’ Validates request payload
  â””â”€ tasks.py â†’ Schedule Celery task, generate task_id, store metadata
      â”‚
      â–¼
Celery Worker
  â”œâ”€ analysis.py â†’ Compute stats, aggregations, correlations
  â”œâ”€ rules.py + rules.yaml â†’ Evaluate configurable business rules
  â”œâ”€ plugins/base.py â†’ Execute ML models or analytics plugins
  â””â”€ Store results â†’ Redis (ephemeral) + MySQL/PostgreSQL (persistent)
```

**Design Rationale:**

* YAML-driven rules â†’ **non-developer configurable**
* Plugin hooks â†’ **modular extensions**
* Storage separation â†’ Redis (progress) vs MySQL (persistent)

---

## 4ï¸âƒ£ Frontend & Dashboard Flow
React components are built to be state-aware, reacting to partial data chunks as they arrive from the WebSocket stream.
```
Dashboard.tsx
  â”‚ Upload CSV & select columns
  â–¼
usePyInsight Hook
  â”œ analyze(file, columns) â†’ POST /api/analyze() â†’ receive task_id
  â”” status(task_id) â†’ Poll GET /api/status/<task_id> or subscribe WS events
       â”‚
Dashboard renders:
  â€¢ Partial summaries in real-time
  â€¢ Rule violations (severity & priority)
  â€¢ Plugin/ML outputs (charts, tables, visualizations)
  â€¢ Interactive filtering, sorting, column selection
```

**Notes:** Reactive, incremental rendering, dynamic visualizations.

---

## 5ï¸âƒ£ WebSocket Event Flow
To achieve "Live" updates, we utilize a Pub/Sub (Publisher/Subscriber) pattern via Redis. This decouples the worker's progress from the API's web server logic.
```
ğŸŸª Celery Worker
      â”‚ Publishes task events â†’ ğŸŸ« Redis (pub/sub)
      â–¼
ğŸŸ¨ Backend Channels (JobConsumer)
      â”‚ Subscribes to Redis events â†’ forwards to WS clients
      â–¼
ğŸŸ§ WebSocket Client
      â”‚ Receives incremental updates
      â–¼
ğŸŸ© Dashboard UI
      â”‚ Updates partial results, highlights violations, renders plugin outputs
```

---

## 6ï¸âƒ£ Docker Deployment Architecture
The entire ecosystem is containerized, ensuring that the development environment perfectly mirrors production.
```
+-----------+    +--------+    +------------+
|  ğŸŸ« MySQL |    | ğŸŸ« Redis|    | ğŸŸ¨ Backend |
| Database  |    | Cache  |    | Django +  |
| 8.x       |    | 7.x    |    | Celery + |
+-----+-----+    +---+----+    | Channels |
      â”‚              â”‚          +-----+-----+
 Docker Compose network connects all containers
      â”‚
ğŸŸ© Frontend React container â†’ Axios + WebSocket
      â”‚
ğŸŸ¦ User Dashboard â†’ interacts with backend
```

* All components containerized â†’ consistent dev/prod environments
* Docker Compose â†’ simplifies local deployment
* Kubernetes-ready â†’ supports horizontal scaling
* CI/CD friendly â†’ reproducible deployments

---

## 7ï¸âƒ£ Dockerfile Examples

### Backend Dockerfile (`backend/Dockerfile`)

```dockerfile
FROM python:3.12-slim

WORKDIR /app

# Install dependencies
COPY backend/requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy code
COPY backend/ .

# Collect static files (if any)
RUN python manage.py collectstatic --noinput

EXPOSE 8000
CMD ["gunicorn", "pyinsight.wsgi:application", "--bind", "0.0.0.0:8000", "--workers", "4"]
```

### Frontend Dockerfile (`frontend/Dockerfile`)

```dockerfile
FROM node:20-alpine

WORKDIR /app

COPY frontend/package.json frontend/package-lock.json ./
RUN npm install

COPY frontend/ .
RUN npm run build

EXPOSE 3000
CMD ["npm", "run", "start"]
```

### Celery Worker Dockerfile (`celery/Dockerfile`)

```dockerfile
FROM python:3.12-slim

WORKDIR /app

COPY backend/requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY backend/ .

CMD ["celery", "-A", "pyinsight", "worker", "--loglevel=info"]
```

---

## 8ï¸âƒ£ docker-compose.yml Example

```yaml
version: "3.9"

services:
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: pyinsight-backend
    command: gunicorn pyinsight.wsgi:application --bind 0.0.0.0:8000 --workers 4
    volumes:
      - ./backend:/app
    ports:
      - "8000:8000"
    depends_on:
      - db
      - redis

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    container_name: pyinsight-frontend
    ports:
      - "3000:3000"
    depends_on:
      - backend

  celery:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: pyinsight-celery
    command: celery -A pyinsight worker --loglevel=info
    depends_on:
      - backend
      - redis
      - db

  redis:
    image: redis:7-alpine
    container_name: pyinsight-redis
    ports:
      - "6379:6379"

  db:
    image: mysql:8.0
    container_name: pyinsight-db
    environment:
      MYSQL_DATABASE: pyinsight
      MYSQL_ROOT_PASSWORD: rootpassword
      MYSQL_USER: pyinsightuser
      MYSQL_PASSWORD: pyinsightpass
    ports:
      - "3306:3306"
```

**Notes:**

* All services share a Docker network â†’ easy communication
* Redis = ephemeral progress/pub-sub, MySQL = persistent storage
* Frontend & backend container ports exposed for local and cloud deployment
* Cloud deployment is seamless using the same images + Kubernetes or ECS

---

## 9ï¸âƒ£ Unified Async Swimlane
This view summarizes the handoff points between every major actor in the system.
```
ğŸŸ¦ USER â†’ ğŸŸ© FRONTEND â†’ ğŸŸ¨ BACKEND â†’ ğŸŸª CELERY â†’ ğŸŸ« STORAGE â†’ ğŸŸ§ WEBSOCKET â†’ ğŸŸ© DASHBOARD
Select CSV/Columns â†’ Upload CSV â†’ Validate & Return task_id â†’ Summarize CSV, Evaluate Rules, Execute Plugins â†’ Redis/MySQL â†’ Push incremental updates â†’ Render partial & final analytics
```

---

## ğŸ”Ÿ Key Takeaways

1. **Decoupled, modular architecture** â†’ easy maintenance & scalability
2. **Asynchronous workflows** â†’ responsive UX even for very large CSVs
3. **Incremental feedback (Redis + WebSocket)** â†’ live dashboards
4. **Flexible rule engine & plugin/ML pipelines** â†’ business-driven customization
5. **Containerized & cloud-ready** â†’ deploy locally via Docker Compose or to cloud clusters

---


Do you want me to generate that diagram next?
